### 前言

- AlexNet经典论文：[ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

- 因为先学习了网络模型，所以再回读论文就会很快，只对论文进行总结阅读

### Abstract

- 我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的top-1和top-5的错误率，这比以前的最先进水平要好得多。
- 具有6000万个参数和65万个神经元的神经网络由五个卷积层，其中紧随着部分层的后面是max-pooling层，和三个全连接层，以及最后的1000维的softmax层组成。
- 为了加快训练速度，我们使用非饱和神经元和非常高效的GPU实现卷积运算。
- 为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法，这种方法被证明是非常有效的。

### 1. Introduction

- 目前的物体识别方法对于机器学习方法是必不可少的。为了改善他们的性能，我们可以收集更大的数据集，学习更强大的模型，并使用更好的技术来防止过度拟合。
- 现实环境中的物体表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。
- 要从数百万图像中识别数千个对象，我们需要一个具有强大学习能力的模型。
- 与具有相当数量的层数的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而理论上的最佳性能可能仅略差一些。
- 不论CNN的性质多有吸引力，也不论它们局部结构的相对效率有多高，但把它们大规模应用于高分辨率图像仍然过于昂贵。
- 当前的GPU搭配了高度优化的二维卷积实现，足以促进大型CNN的训练，最近的数据集（如ImageNet）包含足够的标记示例来训练此类模型，而不会出现严重的过拟合。

- 最后简单介绍了本文的具体贡献。

### 2. DataSet

- 简要介绍ImageNet数据集的基本情况。
- 将输入定为256*256大小，除了从每个像素中减去训练集的平均值之外，我们不以任何其他方式预处理图像。

### 3. The Architecture

- 本章节为框架，该章节作者介绍了AelxNet的创新点。

#### 3.1 ReLU Nonlinearity

- 将神经元的输出标准建模为f（f作为输入x的函数）是：$f(x)=tanh(x)$或者$f(x)=(1+e^{-x})^{-1}$。
- 使用ReLU的深度卷积神经网络比使用tanh单位的同等网络的训练快几倍。
- 这在图1中得到了证明，该图显示了特定四层卷积网络在CIFAR-10数据集上达到25％的训练误差所需的迭代次数。如下图所示：实线为ReLU，虚线为tanh。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/paper/3-1.jpg)

- **将ReLU代替sigmoid作为CNN的激活函数**，并验证了ReLU在较深的网络里性能优于sigmoid，有效解决了sigmoid引起的梯度弥散的问题。而且ReLU比sigmoid的学习速度更快，节省训练时间。

#### 3.2 Training on Multiple GPUs

-  该段主要介绍了在多GPU上训练的优点。
-  **多GPU训练**，由于以前的GPU内存（GTX580内存为3G）较小，这限制了可以在其上训练的网络的最大尺寸。AlexNet将网络分布在两个GPU上，而在训练时GPU仅在特定的层间进行通信，从而减少性能消耗。与在单个GPU上训练且每个卷积层的内核数量少一半的网络相比，这个方案分别将top-1和top-5的错误率分别降低了1.7％和1.2％。双GPU网络的训练时间比单GPU网络的训练时间少一点。

#### 3.3 Local Response Normalization

- ReLU具有所需的属性，它们不需要输入归一化来防止它们饱和。
- 我们仍然发现以下的局部归一化方案有助于泛化。
- $a_{x,y}^i$表示通过在位置(x,y) 处应用内核i然后应用非线性ReLU计算得到的神经元激活度，则响应归一化的激活度$b_{x,y}^i$由以下表达式计算得来：

$$
b_{x, y}^{i}=a_{x, y}^{i} /\left(k+\alpha \sum_{j=\max (0, i-n / 2)}^{\min (N-1, i+n / 2)}\left(a_{x, y}^{j}\right)^{2}\right)^{\beta}
$$

- 其中求和覆盖了n个“相邻的”位于相同空间位置的核图，并且N是该层中的核的总数。
- 内核图的排序当然是任意的，并在训练开始之前就确定好。这种响应归一化实现了一种由真实神经元中发现的类型所激发的横向抑制形式，在使用不同的内核计算的神经元输出之间产生对大激活度的竞争。
- 我们在某些层的非线性ReLU后应用了这种归一化。
- **提出LRN局部响应归一化（用于ReLU后）**，不同的内核计算的神经元输出之间产生对大激活度的竞争，使得局部较大的响应值更大，而小的会变得更小，从而抑制了小的神经元，增强模型的泛化能力。响应归一化将top-1和top-5的错误率分别降低了1.4％和1.2％。

#### 3.4 Overlapping Pooling

- 传统上，相邻的池化单元整合的邻域不重叠。
- 更准确地说，一个池化层可以被认为是由间隔s个像素的池化单元组成的网格，每个池化单元整合一个大小为z×z，以它的位置为中心的邻域。如果我们设定s = z，我们就可以得到CNN中常用的传统的局部池化。

- 如果我们设置s <z，我们得到重叠池化。
- **使用重叠的最大池化代替平均池化**，避免平均池化造成的模糊化效果。而且Alexnet中的步长比池化核的尺寸要小，池化层的输出间会有重叠，这样使特征能表现更多的内容，提高识别性能。这个方案将top-1和top-5的错误率分别降低了0.4％和0.3％。通常在训练期间观察到重叠池模型稍微难以过度拟合。

#### 3.5 Overall Architecture

- 网络包含八个带权值的层;前五个是卷积层，其余三个是全连接层。
- 最后的全连接层的输出被馈送到1000-way softmax，其产生1000个类别标签上的分布。
- 我们的网络使多项逻辑回归（multinomial logistic regression）方程最大化，这相当于在预测分布下最大化正确标签的对数概率（log-probability ）的训练实例的平均值。
- 第二，第四和第五个卷积层的内核只与位于同一GPU上的前一层内核图相连接。如下图所示。
- 第三个卷积层的内核连接到第二层的所有内核图。
- 全连接层中的神经元连接到前一层中的所有神经元。
- 第一和第二卷积层后面都跟着响应归一化层。 
- max-pooling层跟随在响应归一化层以及第五个卷积层后面。
- 非线性ReLU应用到每个卷积和全连接层的输出。

- 第一卷积层用步长为4个像素（这是核图中相邻神经元的感受野中心之间的距离），尺寸为11×11×3的96个内核对224×224×3输入图像进行卷积。
- 第二卷积层将第一卷积层的（响应归一化和池化后的）输出作为输入。并用256个大小为5×5×48的内核对其进行卷积。
- 第三，第四和第五卷积层彼此连接，它们间没任何池化或归一化层。第三卷积层具有连接到第二卷积层的（归一化，池化后的）输出的尺寸为3×3×256的384个内核。第四卷积层有384个大小为3×3×192的内核，第五卷积层有256个大小为3×3×192的内核。
- 全连接层各有4096个神经元。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/paper/3-2.jpg)

### 4. Reducing Overfitting

- 给出对抗过拟合问题的两种方法

#### 4.1 Data Augmentation（数据增强）

- 采用了两种不同的数据增强形式，这两种形式都允许通过很少的计算从原始图像生成变换的图像，所以变换后的图像不需要存储在磁盘上。
- 数据增强的第一种形式是生成图像平移和水平镜像。我们通过从256×256图像中提取随机的224×224块（及其水平镜像），并在这些提取的块上训练我们的网络。这使我们的训练集增加了2048倍，尽管由此产生的训练样例高度相互依赖的。如果没有这个方案，我们的网络会遭受严重的过拟合。
- 数据增强的第二种形式包括改变训练图像中RGB通道的强度值。具体而言，我们在整个ImageNet训练集上的RGB像素值集上做PCA。对于每个训练图像，我们成倍增加已有的主成分，增加的幅度为对应的特征值乘以从一个均值为零，标准差为0.1的高斯分布中提取的随机变量。
- **数据增强。**因为Alexnet的参数量巨多，容易造成过拟合，通过截取，平移，翻转还有RGB像素值集上做PCA（对于每个训练图像，成倍增加已有的主成分）等方法使得数据集更丰富，从而提高泛化能力。从原始图像生成变换的图像是在CPU上的Python代码中生成的，而GPU正在训练上一批图像，这节约了时间。

#### 4.2 Dropout

- 结合许多不同模型的预测是减少测试错误的一个非常成功的方法，但对于已经花费数天训练的大型神经网络来说，它似乎太昂贵了。
- 引入“dropout”，**以0.5的概率把每个隐藏的神经元的输出设置为零**。以这种方式“dropout”的神经元不参与正向传递，也不参与反向传递。所以每次提交输入时，神经网络都采样不同的体系结构。
- 但是所有这些体系结构共享权重。这种技术减少了神经元的复杂的共同适应，因为神经元不能依赖于特定的其他神经元的存在。因此，它被迫网络学习更强大的特征，与其他神经元的许多不同的随机子集结合使用时相当有益处。
- 注意dropout大致使收敛所需的迭代次数翻倍。

### 5. Details of learning

- 该部分介绍了训练部分的参数细节

### 6. Results

### 7. Discussion